{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cffc02f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #for navigation to the data folder \n",
    "import cv2 # image processing library\n",
    "import numpy as np  #Linear Algebra Operations.\n",
    "import pandas as pd   # Data manipulation\n",
    "from sklearn.preprocessing import LabelEncoder#target labelling\n",
    "\n",
    "image_folder = \"D:\\Agricultural_Dataset\"\n",
    "csv_file_path = \"C:/Users/wam4e/OneDrive/Desktop/Features_Reduced.csv\"\n",
    "image_vectors = []#initializing lists \n",
    "labels = []#corresponding label\n",
    "#image preprocessing: resizing, flattening and appending in the list\n",
    "for category_index, category_name in enumerate(os.listdir(image_folder)):  #traverse through parent folder\n",
    "    category_path = os.path.join(image_folder, category_name)   # for every sub-folder in parent folder\n",
    "    \n",
    "    for image_name in os.listdir(category_path):    # traverse through sub-folder\n",
    "        image_path = os.path.join(category_path, image_name) # for each image in subfolder\n",
    "        \n",
    "        # Readinf and resizing all the images to the same shape mentioned.\n",
    "        image = cv2.imread(image_path)  #first read image with cv2 library\n",
    "        image = cv2.resize(image, (224, 224))  #then resize it\n",
    "        \n",
    "        # Vectorizing the image into a vector\n",
    "        image_vector = image.flatten()  #then flatten it\n",
    "        \n",
    "        # Appending the vectorized image and corresponding target value/category to the lists\n",
    "        image_vectors.append(image_vector)  #vectorize it(i.e storing one column under another column)\n",
    "        labels.append(category_index)  #appending the catogerory to that image \n",
    "\n",
    "        \n",
    "image_vectors = np.array(image_vectors)  # with numpy changing into arrays for numerical calucations\n",
    "labels = np.array(labels)  # smae for labels \n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)  # used to transform all numerical values into certain range by dividing all\n",
    "                                                    # input with max among all inputs\n",
    "\n",
    "# Creating a DataFrame with columns 'category' and 'x0', 'x1', 'x2', 'x3'....\n",
    "column_names = ['category'] + [f'x{i}' for i in range(image_vectors.shape[1])]\n",
    "data = np.column_stack((labels_encoded, image_vectors))\n",
    "df = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "# Saving the DataFrame(features and target) to the CSV file whose path is mentined above.\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbbbb3b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Feature Vector =  150528\n",
      "Number of Hidden Layers = 3\n",
      "Size of each Hidden Layer =  256\n",
      "Size of Output Layer =  8\n",
      "learning rate =  0.01\n",
      "Number of iterations =  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wam4e\\AppData\\Local\\Temp\\ipykernel_20664\\4010968531.py:50: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss = 18.80524899074016\n",
      "Iteration 10: Loss = 13.619800476294053\n",
      "Iteration 20: Loss = 10.676067378471437\n",
      "Iteration 30: Loss = 8.794833378307967\n",
      "Iteration 40: Loss = 7.635417035342546\n",
      "Iteration 50: Loss = 6.956629814848293\n",
      "Iteration 60: Loss = 6.502724880637368\n",
      "Iteration 70: Loss = 6.134145244644582\n",
      "Iteration 80: Loss = 5.885606249739013\n",
      "Iteration 90: Loss = 5.677454057898319\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split # this function splits data for traning and testing parts \n",
    "from sklearn.preprocessing import StandardScaler  #it standardises the input and prevents some features from dominating others \n",
    "                                                 #due to their larger range of values.\n",
    "from sklearn.metrics import accuracy_score  #this function is used to calculate accuracy\n",
    "\n",
    "# Load the dataset\n",
    "data = \"C:/Users/wam4e/OneDrive/Desktop/Features_Reduced.csv\"\n",
    "df = pd.read_csv(data)  #loads into dataframe\n",
    "\n",
    "# Preprocess the data\n",
    "X = df.drop(['category'], axis=1)  #X drops category column and now contains only the feature columns\n",
    "y = df['category'].values  #category values are stored in y\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()  #Standardization ensures that all feature values have a mean of zero and a standard deviation of one\n",
    "                        # which is often necessary for machine learning algorithms to work effectively.\n",
    "X = scaler.fit_transform(X) #This step ensures that the features are on the same scale\n",
    "\n",
    "\n",
    "# One-hot vector to predict the category of an input vector.\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    num_samples = len(labels)\n",
    "    encoded_labels = np.zeros((num_samples, num_classes))  # it creates numpy array\n",
    "    for i in range(num_samples):\n",
    "        encoded_labels[i, labels[i]] = 1  #only for that one class it sets to 1\n",
    "    return encoded_labels\n",
    "\n",
    "y = one_hot_encode(y, num_classes=8)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# 85% images -> Training , 15% images -> Testing\n",
    "Training_Data_Features, Testing_Data_Features, Target_Training_Data, Target_Testing_Data = train_test_split(X, y, \n",
    "                                                                                        test_size=0.15, random_state=42)\n",
    "\n",
    "\n",
    "# Initialize weights and biases in all the layers\n",
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    W1 = np.random.randn(hidden_size, input_size) #initilising w1 with random number for first hidden layer\n",
    "    b1 = np.zeros((hidden_size, 1))  # biase can be set to 0 or any small values\n",
    "    W2 = np.random.randn(hidden_size, hidden_size) #initilising w2 with random number for second hidden layer\n",
    "    b2 = np.zeros((hidden_size, 1))\n",
    "    W3 = np.random.randn(output_size, hidden_size)  #initilising w3 with random number for output layer\n",
    "    b3 = np.zeros((output_size, 1))\n",
    "    return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "\n",
    "\n",
    "# Sigmoid activation function used for introducing \"non-linearity\" in \"intermediate\" hidden layers.\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "\n",
    "# Forward propagation (feed-forward to obtain output).\n",
    "def forward_propagation(X, W1, b1, W2, b2, W3, b3):\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = sigmoid(Z1)  #sigmoid used for binary class classification\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    \n",
    "    \n",
    "    # Soft-max function at the output layer.\n",
    "    # \n",
    "    A3 = np.exp(Z3) / np.sum(np.exp(Z3), axis=0)  # soft-max hepls in multi-class classification and provides probability to \n",
    "                                                # which class it belongs\n",
    "    return A1, A2, A3\n",
    "\n",
    "\n",
    "\n",
    "# Compute the cross-entropy loss (function to describe loss/error in multi-class classification)\n",
    "def compute_loss(A3, Y):\n",
    "    m = Y.shape[1]\n",
    "    loss = -1/m * np.sum(Y * np.log(A3))  #cross-entropy formula\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "# Backpropagation with regard to the \"cross-entropy loss function\".\n",
    "def backward_propagation(X, Y, A1, A2, A3, W2, W3):\n",
    "    m = X.shape[1]\n",
    "    # Compute gradients\n",
    "    \n",
    "    # For the last layer\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = (1/m) * np.dot(dZ3, A2.T)\n",
    "    db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    \n",
    "    #For the second-last layer\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = dA2 * A2 * (1 - A2)\n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    #for the the first layer\n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = dA1 * A1 * (1 - A1)\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    # derivatives correswponding to each layer.\n",
    "    return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "\n",
    "\n",
    "# Update parameters using gradient descent algorithm.\n",
    "def update_parameters(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate):\n",
    "    \n",
    "    # Updating weight and bias term in first layer.\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    \n",
    "    # Updating weight and bias in second layer.\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    # Updating weight and bias in third layer.\n",
    "    W3 = W3 - learning_rate * dW3\n",
    "    b3 = b3 - learning_rate * db3\n",
    "    \n",
    "    return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "\n",
    "\n",
    "# Train the neural network\n",
    "def train_neural_network(X, Y, input_size, hidden_size, output_size, learning_rate, num_iterations):\n",
    "    W1, b1, W2, b2, W3, b3 = initialize_parameters(input_size, hidden_size, output_size)\n",
    "    losses = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        A1, A2, A3 = forward_propagation(X, W1, b1, W2, b2, W3, b3)\n",
    "        loss = compute_loss(A3, Y)\n",
    "        dW1, db1, dW2, db2, dW3, db3 = backward_propagation(X, Y, A1, A2, A3, W2, W3)\n",
    "        W1, b1, W2, b2, W3, b3 = update_parameters(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate)\n",
    "        losses.append(loss)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Iteration {i}: Loss = {loss}\")\n",
    "\n",
    "    return W1, b1, W2, b2, W3, b3, losses\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions using the trained model.\n",
    "def predict(X, W1, b1, W2, b2, W3, b3):\n",
    "    _, _, A3 = forward_propagation(X, W1, b1, W2, b2, W3, b3)\n",
    "    predictions = np.argmax(A3, axis=0)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n",
    "# Testing the neural network and calculating accuracy.\n",
    "def test_neural_network(Testing_Data_Features, Target_Testing_Data, W1, b1, W2, b2, W3, b3):\n",
    "    predictions = predict(Testing_Data_Features, W1, b1, W2, b2, W3, b3)\n",
    "    true_labels = np.argmax(Target_Testing_Data, axis=0)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = Training_Data_Features.shape[1]\n",
    "print(\"Size of Feature Vector = \", input_size)\n",
    "print(\"Number of Hidden Layers = 3\")\n",
    "hidden_size = 256\n",
    "print(\"Size of each Hidden Layer = \", hidden_size)\n",
    "output_size = 8\n",
    "print(\"Size of Output Layer = \", output_size)\n",
    "learning_rate = 0.01\n",
    "print(\"learning rate = \", learning_rate)\n",
    "num_iterations = 100\n",
    "print(\"Number of iterations = \", num_iterations)\n",
    "\n",
    "\n",
    "\n",
    "# Train the neural network\n",
    "W1, b1, W2, b2, W3, b3, losses = train_neural_network(Training_Data_Features.T, Target_Training_Data.T, input_size, \n",
    "                                                      hidden_size, output_size, learning_rate, num_iterations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc82ccd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wam4e\\AppData\\Local\\Temp\\ipykernel_20664\\4010968531.py:50: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 6.25%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = test_neural_network(Testing_Data_Features.T, Target_Testing_Data.T, W1, b1, W2, b2, W3, b3)\n",
    "print(\"Test Accuracy: {:.2f}%\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aea7aca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
